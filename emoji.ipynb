{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('us_trial.text', encoding='utf-8') as textfile:\n",
    "    sentences = textfile.readlines()\n",
    "\n",
    "with open('us_trial.labels', encoding='utf-8') as textfile:\n",
    "    emoji = textfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "s = []\n",
    "t = []\n",
    "for sent, emj in zip(sentences,emoji):\n",
    "    s.append(sent)\n",
    "    t.append(int(emj))\n",
    "tweets.append(('sentences', s))\n",
    "tweets.append(('emoji', t))\n",
    "\n",
    "def make_df(tweets):\n",
    "    df_c = pd.DataFrame.from_items(tweets)\n",
    "    df = df_c.loc[:,[\"sentences\",\"emoji\"]]\n",
    "    df.dropna(how=\"any\", inplace=True)   \n",
    "    #print(df.head)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_df(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           sentences emoji\n",
      "0  Elise has really changed in 3 years!! @ Kill D...    16\n",
      "1  Sunday Sushi @user 4:30pm - 8:30pm come see me...    16\n",
      "2  AMAZING FATHERS DAY! Got to spend it with my l...     0\n",
      "3  Daydreamin' #SecretStairs #MichelTorena #Silve...    13\n",
      "4  This guy . Hanging out in castles 'n shit #sou...     1\n",
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "def sampling_dataset(df):\n",
    "    count = 5000\n",
    "    class_df_sampled = pd.DataFrame(columns = [\"sentences\",\"emoji\"])\n",
    "    temp = []\n",
    "    for c in df:\n",
    "        class_indexes = df[c].index\n",
    "        random_indexes = np.random.choice(class_indexes, count, replace=False)\n",
    "        temp.append(df.loc[random_indexes])\n",
    "        \n",
    "    for each_df in temp:\n",
    "        class_df_sampled = pd.concat([class_df_sampled,each_df],axis=0)\n",
    "    \n",
    "    return class_df_sampled\n",
    "\n",
    "df = sampling_dataset(df)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "w = re.compile(\"\\w+\",re.I)\n",
    "\n",
    "def label_sentences(df):\n",
    "    labeled_sentences = []\n",
    "    for index, datapoint in df.iterrows():\n",
    "        tokenized_words = re.findall(w,datapoint[\"sentences\"].lower())\n",
    "        labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['SENT_%s' %index]))\n",
    "    return labeled_sentences\n",
    "\n",
    "def train_doc2vec_model(labeled_sentences):\n",
    "    model = Doc2Vec(alpha=0.025, min_alpha=0.025)\n",
    "    model.build_vocab(labeled_sentences)\n",
    "    for epoch in range(10):\n",
    "        model.train(labeled_sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.alpha -= 0.002 \n",
    "        model.min_alpha = model.alpha\n",
    "    \n",
    "    return model\n",
    "\n",
    "sen = label_sentences(df)\n",
    "model = train_doc2vec_model(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           sentences emoji  \\\n",
      "0  Elise has really changed in 3 years!! @ Kill D...    16   \n",
      "1  Sunday Sushi @user 4:30pm - 8:30pm come see me...    16   \n",
      "\n",
      "                                 vectorized_comments  \n",
      "0  [0.094514, 0.367859, -0.140067, -0.255143, -0....  \n",
      "1  [-0.016482, 0.173833, -0.0960274, -0.109512, 0...  \n"
     ]
    }
   ],
   "source": [
    "def vectorize_comments(df,d2v_model):\n",
    "    y = []\n",
    "    comments = []\n",
    "    for i in range(0,df.shape[0]):\n",
    "        label = 'SENT_%s' %i\n",
    "        comments.append(d2v_model.docvecs[label])\n",
    "    df['vectorized_comments'] = comments\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = vectorize_comments(df,model)\n",
    "print (df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X,y):\n",
    "    n_estimators = [200,400]\n",
    "    min_samples_split = [2]\n",
    "    min_samples_leaf = [1]\n",
    "    bootstrap = [True]\n",
    "\n",
    "    parameters = {'n_estimators': n_estimators, 'min_samples_leaf': min_samples_leaf,\n",
    "                  'min_samples_split': min_samples_split}\n",
    "\n",
    "    clf = GridSearchCV(RFC(verbose=1,n_jobs=4), cv=4, param_grid=parameters)\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "#print(len(df['emoji']))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"vectorized_comments\"].tolist(), df['emoji'], test_size=0.3, random_state=33)\n",
    "\n",
    "#print(df[\"vectorized_comments\"].tolist())\n",
    "#classifier = train_classifier(X_train,y_train)\n",
    "#print (classifier.best_score_)\n",
    "#print (classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-5faf68185a11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf_logreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclf_logreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m   1216\u001b[0m                          order=\"C\")\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "clf_logreg = linear_model.LogisticRegression(random_state = 42)\n",
    "clf_logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
